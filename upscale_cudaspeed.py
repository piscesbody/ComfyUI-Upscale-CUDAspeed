"""
ComfyUI Upscale CUDAspeed
ä¼˜åŒ–ç¼–è¯‘åé•¿æ—¶é—´å¤„ç†å’Œå°ºå¯¸å˜åŒ–é‡æ–°ç¼–è¯‘çš„é—®é¢˜
"""

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from spandrel import ModelLoader, ImageModelDescriptor
from comfy import model_management
import comfy.utils
import folder_paths
from typing import Optional, Tuple, List
import math
import time
import gc
import os
import pickle
import hashlib

try:
    from tqdm import tqdm
    tqdm_available = True
except ImportError:
    tqdm_available = False
    print("tqdm not available, using basic progress indicators")

try:
    from spandrel_extra_arches import EXTRA_REGISTRY
    from spandrel import MAIN_REGISTRY
    MAIN_REGISTRY.add(*EXTRA_REGISTRY)
    logging.info("æˆåŠŸå¯¼å…¥spandrel_extra_archesï¼šæ”¯æŒéå•†ä¸šæ”¾å¤§æ¨¡å‹ã€‚")
except:
    pass

class UpscaleModelLoader:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": { "model_name": (folder_paths.get_filename_list("upscale_models"), ),
                             }}
    RETURN_TYPES = ("UPSCALE_MODEL",)
    FUNCTION = "load_model"
    CATEGORY = "loaders"

    def load_model(self, model_name):
        model_path = folder_paths.get_full_path_or_raise("upscale_models", model_name)
        sd = comfy.utils.load_torch_file(model_path, safe_load=True)
        if "module.layers.0.residual_group.blocks.0.norm1.weight" in sd:
            sd = comfy.utils.state_dict_prefix_replace(sd, {"module.":""})
        out = ModelLoader().load_from_state_dict(sd).eval()

        if not isinstance(out, ImageModelDescriptor):
            raise Exception("æ”¾å¤§æ¨¡å‹å¿…é¡»æ˜¯å•å›¾åƒæ¨¡å‹ã€‚")

        return (out, )


class ImageUpscaleWithModelCUDAspeedFixed:
    """é«˜æ€§èƒ½æ”¾å¤§èŠ‚ç‚¹
    ä¼˜åŒ–ç¼–è¯‘åé•¿æ—¶é—´å¤„ç†å’Œå°ºå¯¸å˜åŒ–é‡æ–°ç¼–è¯‘çš„é—®é¢˜
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "upscale_model": ("UPSCALE_MODEL",),
                "image": ("IMAGE",),
                "use_autocast": (["enable", "disable"], {"default": "enable"}),
                "precision": (["auto", "fp16", "fp32", "bf16"], {"default": "auto"}),
                "tile_size": ("INT", {"default": 0, "min": 0, "max": 2048, "step": 64}),
                "overlap": ("INT", {"default": 0, "min": 0, "max": 128, "step": 8}),
                "enable_compile": (["enable", "disable"], {"default": "enable"}),
                "optimization_level": (["balanced", "speed", "memory"], {"default": "balanced"}),
            },
            "optional": {
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 16, "step": 1}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "upscale"
    CATEGORY = "image/upscaling"
    
    # ç¼–è¯‘åçš„æ¨¡å‹ç¼“å­˜ï¼ˆç±»å˜é‡ï¼Œåœ¨å®ä¾‹é—´å…±äº«ï¼‰
    _compiled_models = {}
    
    # ç¼–è¯‘æ¨¡å‹å­˜å‚¨ç›®å½•ï¼ˆç”¨äºè®°å½•ç¼–è¯‘çŠ¶æ€ï¼‰
    _compiled_models_dir = None
    
    # è¿è¡Œæ—¶ç¼–è¯‘ç¼“å­˜ï¼ˆç±»å˜é‡ï¼Œåœ¨å®ä¾‹é—´å…±äº«ï¼‰
    _runtime_compiled_models = {}
    
    # å°ºå¯¸ç¼–è¯‘ç¼“å­˜ - å…³é”®ä¿®å¤ï¼šä¸ºä¸åŒå°ºå¯¸ç¼“å­˜ç¼–è¯‘ç»“æœ
    _size_compiled_models = {}
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹å­˜å‚¨ç›®å½•"""
        # è®¾ç½®ç¼–è¯‘æ¨¡å‹å­˜å‚¨ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        self._compiled_models_dir = os.path.join(current_dir, "compiled_models")
        os.makedirs(self._compiled_models_dir, exist_ok=True)
        print(f"ğŸ“ ç¼–è¯‘æ¨¡å‹å­˜å‚¨ç›®å½•: {self._compiled_models_dir}")
        
        # åˆå§‹åŒ–è¿è¡Œæ—¶ç¼“å­˜ï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å®ä¾‹åŒ–ï¼‰
        if not hasattr(ImageUpscaleWithModelCUDAspeedFixed, '_runtime_compiled_models'):
            ImageUpscaleWithModelCUDAspeedFixed._runtime_compiled_models = {}
        
        # åˆå§‹åŒ–å°ºå¯¸ç¼–è¯‘ç¼“å­˜
        if not hasattr(ImageUpscaleWithModelCUDAspeedFixed, '_size_compiled_models'):
            ImageUpscaleWithModelCUDAspeedFixed._size_compiled_models = {}
        
        # åŠ è½½å·²ç¼–è¯‘æ¨¡å‹è®°å½•
        self._load_compiled_models_info()
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºè¿è¡Œæ—¶ç¼“å­˜çŠ¶æ€
        print(f"ğŸ” è¿è¡Œæ—¶ç¼“å­˜çŠ¶æ€: {len(ImageUpscaleWithModelCUDAspeedFixed._runtime_compiled_models)} ä¸ªç¼–è¯‘æ¨¡å‹")
        print(f"ğŸ” å°ºå¯¸ç¼“å­˜çŠ¶æ€: {len(ImageUpscaleWithModelCUDAspeedFixed._size_compiled_models)} ä¸ªå°ºå¯¸ç¼“å­˜")
    
    def _get_model_hash(self, model_state_dict):
        """ç”Ÿæˆæ¨¡å‹çŠ¶æ€å­—å…¸çš„å“ˆå¸Œå€¼"""
        # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ¨¡å‹çŠ¶æ€ç”¨äºå“ˆå¸Œè®¡ç®—
        simplified_state = {}
        for key, value in model_state_dict.items():
            # åªå–éƒ¨åˆ†å…³é”®å‚æ•°è®¡ç®—å“ˆå¸Œï¼Œé¿å…è®¡ç®—é‡è¿‡å¤§
            if 'weight' in key or 'bias' in key:
                # å–å‰100ä¸ªå…ƒç´ è®¡ç®—å“ˆå¸Œ
                flat_value = value.flatten()
                sample_size = min(100, len(flat_value))
                simplified_state[key] = flat_value[:sample_size].cpu().numpy().tobytes()
        
        # è®¡ç®—å“ˆå¸Œ
        hash_obj = hashlib.md5()
        for key in sorted(simplified_state.keys()):
            hash_obj.update(simplified_state[key])
        
        return hash_obj.hexdigest()
    
    def _get_compiled_model_path(self, model_hash):
        """è·å–ç¼–è¯‘æ¨¡å‹ä¿¡æ¯æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self._compiled_models_dir, f"compiled_{model_hash}.pkl")
    
    def _load_compiled_models_info(self):
        """åŠ è½½å·²ç¼–è¯‘æ¨¡å‹ä¿¡æ¯ï¼ˆä»…è®°å½•ï¼Œä¸åŠ è½½ç¼–è¯‘å‡½æ•°ï¼‰"""
        print("ğŸ” æ£€æŸ¥å·²ç¼–è¯‘çš„æ¨¡å‹ä¿¡æ¯...")
        loaded_count = 0
        
        for filename in os.listdir(self._compiled_models_dir):
            if filename.startswith("compiled_") and filename.endswith(".pkl"):
                file_path = os.path.join(self._compiled_models_dir, filename)
                try:
                    # åªæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œä¸å®é™…åŠ è½½ç¼–è¯‘å‡½æ•°
                    if os.path.getsize(file_path) > 0:
                        # åªè®°å½•æ¨¡å‹å“ˆå¸Œï¼Œä¸åŠ è½½ç¼–è¯‘å‡½æ•°
                        model_hash = filename.replace("compiled_", "").replace(".pkl", "")
                        self._compiled_models[model_hash] = True  # æ ‡è®°ä¸ºå·²ç¼–è¯‘
                        loaded_count += 1
                        print(f"  âœ… å‘ç°ç¼–è¯‘æ¨¡å‹è®°å½•: {filename}")
                        
                except Exception as e:
                    print(f"  âŒ æ£€æŸ¥ç¼–è¯‘æ¨¡å‹å¤±è´¥ {filename}: {e}")
        
        print(f"ğŸ“Š å‘ç° {loaded_count} ä¸ªç¼–è¯‘æ¨¡å‹è®°å½•")
    
    def _save_compiled_model_info(self, model_hash):
        """ä¿å­˜ç¼–è¯‘æ¨¡å‹ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆä¸ä¿å­˜å®é™…çš„ç¼–è¯‘å‡½æ•°ï¼‰"""
        try:
            # ä¸ä¿å­˜ç¼–è¯‘å‡½æ•°æœ¬èº«ï¼Œåªä¿å­˜ç¼–è¯‘è®°å½•
            compiled_data = {
                'model_hash': model_hash,
                'save_time': time.time(),
                'compile_info': 'æ¨¡å‹å·²ç¼–è¯‘ï¼Œç¼–è¯‘å‡½æ•°æ— æ³•åºåˆ—åŒ–ä¿å­˜'
            }
            
            file_path = self._get_compiled_model_path(model_hash)
            with open(file_path, 'wb') as f:
                pickle.dump(compiled_data, f)
            
            print(f"ğŸ’¾ ç¼–è¯‘æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {os.path.basename(file_path)}")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼–è¯‘æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return False

    def upscale(self, upscale_model, image, use_autocast="enable", precision="auto",
                tile_size=0, overlap=0, enable_compile="enable", optimization_level="balanced",
                batch_size=1):
        
        print(f"ğŸš€ å¼€å§‹å›¾åƒæ”¾å¤§å¤„ç†")
        print(f"ğŸ“Š è¾“å…¥å›¾åƒå°ºå¯¸: {image.shape}")
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_name = self._get_model_name(upscale_model)
        print(f"ğŸ”§ ä½¿ç”¨æ”¾å¤§æ¨¡å‹: {model_name}, æ¨¡å‹ç¼©æ”¾æ¯”ä¾‹: {upscale_model.scale}")
        print(f"âš™ï¸ ä½¿ç”¨å‚æ•° - è‡ªåŠ¨æ··åˆç²¾åº¦: {use_autocast}, ç²¾åº¦: {precision}")
        print(f"ğŸ”§ ä¼˜åŒ–çº§åˆ«: {optimization_level}, æ¨¡å‹ç¼–è¯‘: {enable_compile}")
        
        # è¯¦ç»†æ€§èƒ½ç›‘æ§
        total_start_time = time.time()
        phase_start_time = total_start_time
        
        # ç¡®å®šç²¾åº¦å’Œä¼˜åŒ–è®¾ç½®
        dtype, autocast_enabled = self._determine_precision(precision, use_autocast)
        phase_end_time = time.time()
        print(f"â±ï¸ ç²¾åº¦è®¾ç½®å®Œæˆ - è€—æ—¶: {phase_end_time - phase_start_time:.3f}ç§’")
        phase_start_time = phase_end_time
        
        # æ™ºèƒ½å‚æ•°è®¡ç®—
        tile_size, overlap = self._calculate_optimal_tile_size(
            image.shape, upscale_model.scale, tile_size, overlap, optimization_level
        )
        phase_end_time = time.time()
        print(f"â±ï¸ å‚æ•°è®¡ç®—å®Œæˆ - è€—æ—¶: {phase_end_time - phase_start_time:.3f}ç§’")
        phase_start_time = phase_end_time
        
        print(f"ğŸ“ ä¼˜åŒ–å‚æ•° - ç“¦ç‰‡å¤§å°: {tile_size}, é‡å : {overlap}")
        
        # æ‰§è¡Œæ”¾å¤§å¤„ç†
        result = self._upscale_fixed(
            upscale_model, image, dtype, autocast_enabled,
            tile_size, overlap, enable_compile, batch_size
        )
        
        # æ€§èƒ½ç»Ÿè®¡
        total_end_time = time.time()
        processing_time = total_end_time - total_start_time
        print(f"âœ… å›¾åƒæ”¾å¤§å¤„ç†å®Œæˆ - æ€»è€—æ—¶: {processing_time:.2f}ç§’")
        print(f"ğŸ“Š è¾“å‡ºå›¾åƒå°ºå¯¸: {result[0].shape}")
        
        return result

    def _get_model_name(self, upscale_model):
        """è·å–æ¨¡å‹åç§°ä¿¡æ¯"""
        model_name = getattr(upscale_model, 'name', None)
        if model_name is None:
            model_name = getattr(upscale_model, '__class__', type(upscale_model)).__name__
            if hasattr(upscale_model, 'model'):
                underlying_model = getattr(upscale_model.model, '__class__', None)
                if underlying_model:
                    model_name = f"{model_name}({underlying_model.__name__})"
            else:
                model_name = type(upscale_model).__name__
        return model_name

    def _determine_precision(self, precision, use_autocast):
        """ç¡®å®šç²¾åº¦è®¾ç½®"""
        if precision == "auto":
            if model_management.should_use_fp16():
                precision = "fp16"
            else:
                precision = "fp32"
        
        dtype = torch.float32
        autocast_enabled = False
        
        if use_autocast == "enable":
            if precision == "fp16":
                dtype = torch.float16
                autocast_enabled = True
            elif precision == "bf16":
                dtype = torch.bfloat16
                autocast_enabled = True
        
        return dtype, autocast_enabled

    def _calculate_optimal_tile_size(self, image_shape, scale_factor, tile_size, overlap, optimization_level):
        """æ™ºèƒ½è®¡ç®—æœ€ä¼˜ç“¦ç‰‡å¤§å°å’Œé‡å """
        _, _, height, width = image_shape if len(image_shape) == 4 else (1, *image_shape[1:])
        
        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†å‚æ•°ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å€¼
        if tile_size > 0 and overlap > 0:
            return tile_size, overlap
        
        # æ ¹æ®ä¼˜åŒ–çº§åˆ«è®¡ç®—é»˜è®¤å€¼
        if optimization_level == "speed":
            base_tile = 512  # ä¼˜åŒ–ï¼šå‡å°é»˜è®¤ç“¦ç‰‡å¤§å°ï¼Œé¿å…è¿‡å¤§ç“¦ç‰‡å¯¼è‡´æ€§èƒ½ä¸‹é™
            base_overlap = 16
        elif optimization_level == "memory":
            base_tile = 256   # å°ç“¦ç‰‡ï¼ŒèŠ‚çœå†…å­˜
            base_overlap = 24
        else:  # balanced
            base_tile = 384
            base_overlap = 32
        
        # æ ¹æ®å›¾åƒå°ºå¯¸æ™ºèƒ½è°ƒæ•´ç“¦ç‰‡å¤§å°
        max_dim = max(height, width)
        
        # ä¼˜åŒ–ï¼šæ›´æ™ºèƒ½çš„ç“¦ç‰‡å¤§å°è®¡ç®—
        if max_dim <= 512:
            tile_size = min(512, base_tile)
        elif max_dim <= 1024:
            tile_size = min(512, base_tile)  # å¯¹äº1080pä»¥ä¸‹å›¾åƒï¼Œä½¿ç”¨512ç“¦ç‰‡
        elif max_dim <= 1920:
            tile_size = min(640, base_tile)  # å¯¹äº2Kå›¾åƒï¼Œä½¿ç”¨640ç“¦ç‰‡
        else:
            tile_size = base_tile
        
        # ä¼˜åŒ–ï¼šæ ¹æ®å®é™…å›¾åƒå°ºå¯¸è¿›ä¸€æ­¥è°ƒæ•´
        # å¦‚æœå›¾åƒå°ºå¯¸å°äºç“¦ç‰‡å¤§å°ï¼Œç›´æ¥ä½¿ç”¨å›¾åƒå°ºå¯¸
        if height < tile_size and width < tile_size:
            tile_size = max(height, width)
        
        # æ ¹æ®ç¼©æ”¾æ¯”ä¾‹è°ƒæ•´é‡å 
        overlap = max(8, base_overlap // max(1, int(scale_factor)))
        
        print(f"ğŸ”§ æ™ºèƒ½ç“¦ç‰‡è®¡ç®— - å›¾åƒå°ºå¯¸: {width}x{height}, è®¡ç®—ç“¦ç‰‡: {tile_size}x{tile_size}, é‡å : {overlap}")
        
        return tile_size, overlap

    def _upscale_fixed(self, upscale_model, image, dtype, autocast_enabled,
                      tile_size, overlap, enable_compile, batch_size):
        """ä¿®å¤æ€§èƒ½é—®é¢˜çš„å•GPUæ”¾å¤§å®ç°"""
        device = model_management.get_torch_device()
        print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - _upscale_fixedå…¥å£: è¾“å…¥å›¾åƒè®¾å¤‡={image.device}")
        
        # å…ˆå°†åŸå§‹æ¨¡å‹ç§»åˆ°è®¾å¤‡
        upscale_model.to(device)
        
        # å‡†å¤‡ç¼–è¯‘æ¨¡å‹ - å…³é”®ä¿®å¤ï¼šä½¿ç”¨å°ºå¯¸æ„ŸçŸ¥çš„ç¼–è¯‘ç¼“å­˜
        use_compiled_model = False
        compiled_forward = None
        
        # ç”Ÿæˆå°ºå¯¸é”®ç”¨äºç¼“å­˜
        size_key = f"{image.shape[2]}x{image.shape[3]}"  # é«˜åº¦xå®½åº¦
        print(f"ğŸ“ å½“å‰è¾“å…¥å°ºå¯¸: {size_key}")
        
        if enable_compile == "enable" and hasattr(torch, 'compile'):
            # è·å–æ¨¡å‹å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†
            model_hash = None
            try:
                if hasattr(upscale_model, 'model') and hasattr(upscale_model.model, 'state_dict'):
                    model_state_dict = upscale_model.model.state_dict()
                    model_hash = self._get_model_hash(model_state_dict)
                    print(f"ğŸ”‘ æ¨¡å‹å“ˆå¸Œ: {model_hash}")
            except Exception as e:
                print(f"âš ï¸ è·å–æ¨¡å‹å“ˆå¸Œå¤±è´¥: {e}")
                model_hash = None
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼–è¯‘è®°å½•
            has_compile_record = model_hash and model_hash in self._compiled_models
            
            # ä½¿ç”¨å°ºå¯¸æ„ŸçŸ¥çš„ç¼“å­˜é”®
            model_key = f"{model_hash}_{size_key}" if model_hash else f"{id(upscale_model)}_{size_key}"
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºç¼“å­˜æŸ¥æ‰¾çŠ¶æ€
            print(f"ğŸ” ç¼“å­˜æŸ¥æ‰¾ - æ¨¡å‹é”®: {model_key}")
            print(f"ğŸ” è¿è¡Œæ—¶ç¼“å­˜ä¸­å­˜åœ¨: {model_key in ImageUpscaleWithModelCUDAspeedFixed._runtime_compiled_models}")
            print(f"ğŸ” å°ºå¯¸ç¼“å­˜ä¸­å­˜åœ¨: {model_key in ImageUpscaleWithModelCUDAspeedFixed._size_compiled_models}")
            print(f"ğŸ” ç¼–è¯‘è®°å½• - æ¨¡å‹å“ˆå¸Œ: {model_hash}, è®°å½•å­˜åœ¨: {has_compile_record}")
            
            # å…³é”®ä¿®å¤ï¼šä¼˜å…ˆæ£€æŸ¥å°ºå¯¸ç¼“å­˜
            if model_key in ImageUpscaleWithModelCUDAspeedFixed._size_compiled_models:
                # ä½¿ç”¨å°ºå¯¸ç¼“å­˜çš„ç¼–è¯‘æ¨¡å‹
                compiled_forward = ImageUpscaleWithModelCUDAspeedFixed._size_compiled_models[model_key]
                use_compiled_model = True
                print(f"âœ… ä½¿ç”¨å·²ç¼–è¯‘æ¨¡å‹ (å°ºå¯¸ç¼“å­˜: {size_key})")
            elif model_key in ImageUpscaleWithModelCUDAspeedFixed._runtime_compiled_models:
                # ä½¿ç”¨è¿è¡Œæ—¶ç¼“å­˜çš„ç¼–è¯‘æ¨¡å‹
                compiled_forward = ImageUpscaleWithModelCUDAspeedFixed._runtime_compiled_models[model_key]
                use_compiled_model = True
                print(f"âœ… ä½¿ç”¨å·²ç¼–è¯‘æ¨¡å‹ (è¿è¡Œæ—¶ç¼“å­˜: {size_key})")
            else:
                # éœ€è¦é‡æ–°ç¼–è¯‘
                if has_compile_record:
                    print(f"ğŸ”§ é‡æ–°ç¼–è¯‘æ¨¡å‹ (å·²æœ‰è®°å½•ï¼Œä½†å°ºå¯¸ {size_key} æœªç¼“å­˜)...")
                else:
                    print(f"ğŸ”§ ç¼–è¯‘æ¨¡å‹ä»¥ä¼˜åŒ–æ€§èƒ½ (å°ºå¯¸: {size_key})...")
                
                try:
                    # å°è¯•ç¼–è¯‘æ¨¡å‹çš„forwardæ–¹æ³•
                    if hasattr(upscale_model, 'model') and hasattr(upscale_model.model, 'forward'):
                        # ä½¿ç”¨æœ€å®‰å…¨çš„ç¼–è¯‘é…ç½®ï¼Œå®Œå…¨é¿å…CUDAå›¾é—®é¢˜
                        import os
                        os.environ["TORCHINDUCTOR_CUDAGRAPHS"] = "0"
                        torch._inductor.config.triton.cudagraphs = False
                        torch._inductor.config.triton.cudagraph_trees = False
                        
                        # ç®€åŒ–çš„ç¼–è¯‘è¿‡ç¨‹ - ç§»é™¤å¤æ‚çš„è¿›åº¦æ¡
                        print("ğŸ”„ å¼€å§‹æ¨¡å‹ç¼–è¯‘... (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)")
                        compile_start_time = time.time()
                        
                        # ä½¿ç”¨æœ€ç®€å•çš„ç¼–è¯‘æ¨¡å¼
                        compiled_forward = torch.compile(
                            upscale_model.model.forward,
                            mode="default",
                            fullgraph=False,
                            dynamic=False  # å›ºå®šå°ºå¯¸ç¼–è¯‘ï¼Œæ€§èƒ½æ›´å¥½
                        )
                        
                        compile_end_time = time.time()
                        compile_time = compile_end_time - compile_start_time
                        
                        print(f"âœ… ç¼–è¯‘å®Œæˆ - è€—æ—¶: {compile_time:.2f}ç§’")
                        
                        # å…³é”®ä¿®å¤ï¼šåŒæ—¶ä¿å­˜åˆ°è¿è¡Œæ—¶ç¼“å­˜å’Œå°ºå¯¸ç¼“å­˜
                        ImageUpscaleWithModelCUDAspeedFixed._runtime_compiled_models[model_key] = compiled_forward
                        ImageUpscaleWithModelCUDAspeedFixed._size_compiled_models[model_key] = compiled_forward
                        
                        # ä¿å­˜ç¼–è¯‘è®°å½•ï¼ˆä¸ä¿å­˜ç¼–è¯‘å‡½æ•°æœ¬èº«ï¼‰
                        if model_hash and not has_compile_record:
                            self._compiled_models[model_hash] = True
                            self._save_compiled_model_info(model_hash)
                            print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸå¹¶å·²è®°å½•")
                        else:
                            print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸ")
                        
                        use_compiled_model = True
                        
                    else:
                        print("âš ï¸ æ¨¡å‹ç»“æ„ä¸æ”¯æŒç¼–è¯‘ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼")
                        use_compiled_model = False
                except Exception as e:
                    print(f"âš ï¸ æ¨¡å‹ç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼: {e}")
                    use_compiled_model = False
        
        # å¯ç”¨Tensor Coreä¼˜åŒ–
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cuda.matmul.allow_tf32 = True
        
        # åˆ›å»ºä¼˜åŒ–çš„CUDAæµ
        compute_stream = torch.cuda.Stream(device)
        data_stream = torch.cuda.Stream(device)
        
        # å¼‚æ­¥æ•°æ®é¢„å¤„ç†ï¼šåœ¨ç¼–è¯‘æ¨¡å‹çš„åŒæ—¶å‡†å¤‡è¾“å…¥æ•°æ®
        print("ğŸ”„ å¼€å§‹å¼‚æ­¥æ•°æ®é¢„å¤„ç†...")
        data_prep_start = time.time()
        
        # å‡†å¤‡è¾“å…¥å›¾åƒï¼ˆå¼‚æ­¥ï¼‰
        with torch.cuda.stream(data_stream):
            in_img = image.movedim(-1, -3).to(device, non_blocking=True)
        
        data_prep_end = time.time()
        print(f"â±ï¸ æ•°æ®é¢„å¤„ç†å®Œæˆ - è€—æ—¶: {data_prep_end - data_prep_start:.2f}ç§’")
        
        # å†…å­˜ç®¡ç†
        print("ğŸ”„ å¼€å§‹å†…å­˜ä¼˜åŒ–...")
        memory_start = time.time()
        self._optimize_memory_usage(upscale_model, in_img, tile_size, device)
        memory_end = time.time()
        print(f"â±ï¸ å†…å­˜ä¼˜åŒ–å®Œæˆ - è€—æ—¶: {memory_end - memory_start:.2f}ç§’")
        
        # ç­‰å¾…æ•°æ®é¢„å¤„ç†å®Œæˆ
        print("ğŸ”„ ç­‰å¾…æ•°æ®é¢„å¤„ç†å®Œæˆ...")
        data_stream.synchronize()
        
        # æ‰§è¡Œæ”¾å¤§å¤„ç†
        try:
            result = self._process_tiles_fixed(
                upscale_model, compiled_forward, use_compiled_model, in_img,
                autocast_enabled, dtype, tile_size, overlap, compute_stream,
                data_stream, batch_size, device
            )
            
            # æ™ºèƒ½æ˜¾å­˜ç®¡ç†ï¼šæ ¹æ®æ˜¾å­˜æƒ…å†µå†³å®šè¾“å‡ºè®¾å¤‡
            result = self._smart_memory_management(result, upscale_model, device)
            
        finally:
            # æ¸…ç†å†…å­˜
            upscale_model.to("cpu")
            if hasattr(torch.cuda, 'empty_cache'):
                torch.cuda.empty_cache()
        
        return result

    def _optimize_memory_usage(self, upscale_model, image, tile_size, device):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        # è®¡ç®—å†…å­˜éœ€æ±‚
        memory_required = model_management.module_size(upscale_model.model)
        memory_required += (tile_size * tile_size * 3) * image.element_size() * 384.0
        memory_required += image.nelement() * image.element_size()
        
        # é‡Šæ”¾å†…å­˜
        model_management.free_memory(memory_required, device)
        
        # é¢„åˆ†é…GPUå†…å­˜æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(torch.cuda, 'memory_allocated'):
            current_allocated = torch.cuda.memory_allocated(device)
            print(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {current_allocated / 1024**3:.2f} GB")

    def _process_tiles_fixed(self, upscale_model, compiled_forward, use_compiled_model, in_img,
                           autocast_enabled, dtype, tile_size, overlap, compute_stream,
                           data_stream, batch_size, device):
        """ä¿®å¤çš„ç“¦ç‰‡å¤„ç† - ç®€åŒ–æµç¨‹ï¼Œç§»é™¤ä¸å¿…è¦çš„é¢„çƒ­"""
        print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - _process_tiles_fixedå…¥å£: è¾“å…¥å›¾åƒè®¾å¤‡={in_img.device}")
        oom = True
        current_tile_size = tile_size
        max_retries = 3
        retry_count = 0
        
        while oom and retry_count < max_retries:
            try:
                # è®¡ç®—å¤„ç†æ­¥éª¤
                steps = in_img.shape[0] * comfy.utils.get_tiled_scale_steps(
                    in_img.shape[3], in_img.shape[2],
                    tile_x=current_tile_size, tile_y=current_tile_size,
                    overlap=overlap
                )
                print(f"ğŸ“ˆ é¢„è®¡å¤„ç†æ­¥éª¤æ•°: {steps}, å½“å‰ç“¦ç‰‡å¤§å°: {current_tile_size}x{current_tile_size}")
                
                # åˆ›å»ºè¿›åº¦æ¡
                pbar = self._create_progress_bar(steps)
                
                # ä¼˜åŒ–çš„æ”¾å¤§å‡½æ•° - æ”¯æŒç¼–è¯‘å’Œæ™®é€šæ¨¡å¼
                def upscale_fn(x):
                    with torch.cuda.stream(compute_stream):
                        if use_compiled_model and compiled_forward is not None:
                            # ä½¿ç”¨ç¼–è¯‘åçš„forwardå‡½æ•°
                            if autocast_enabled:
                                with torch.autocast(device_type="cuda", dtype=dtype):
                                    # ç¼–è¯‘åçš„å‡½æ•°å·²ç»ç»‘å®šäº†æ¨¡å‹å®ä¾‹
                                    result = compiled_forward(x)
                            else:
                                result = compiled_forward(x)
                        else:
                            # ä½¿ç”¨åŸå§‹æ¨¡å‹
                            if autocast_enabled:
                                with torch.autocast(device_type="cuda", dtype=dtype):
                                    result = upscale_model(x)
                            else:
                                result = upscale_model(x)
                        
                        # ç¡®ä¿è¾“å‡ºæ•°æ®ç±»å‹æ­£ç¡®
                        if autocast_enabled and result.dtype != torch.float32:
                            result = result.float()
                    
                    compute_stream.synchronize()
                    return result
                
                # ä½¿ç”¨ä¼˜åŒ–çš„ç“¦ç‰‡ç¼©æ”¾
                print("ğŸ”„ å¼€å§‹tiled_scaleå¤„ç†...")
                print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - tiled_scaleè°ƒç”¨å‰: è¾“å…¥è®¾å¤‡={in_img.device}")
                tiled_scale_start_time = time.time()
                
                # æ‰§è¡Œå®é™…çš„tiled_scaleå¤„ç†
                with torch.no_grad():
                    s = comfy.utils.tiled_scale(
                        in_img,
                        upscale_fn,
                        tile_x=current_tile_size,
                        tile_y=current_tile_size,
                        overlap=overlap,
                        upscale_amount=upscale_model.scale,
                        output_device=device,  # å…³é”®ä¼˜åŒ–ï¼šç›´æ¥è¾“å‡ºåˆ°GPUï¼Œé¿å…ä¸å¿…è¦çš„CPUä¼ è¾“
                        pbar=pbar
                    )
                
                tiled_scale_end_time = time.time()
                print(f"âœ… tiled_scaleå¤„ç†å®Œæˆ - è€—æ—¶: {tiled_scale_end_time - tiled_scale_start_time:.3f}ç§’")
                print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - tiled_scaleè°ƒç”¨å: è¾“å‡ºè®¾å¤‡={s.device}")
                
                oom = False
                
                # å…³é—­è¿›åº¦æ¡
                if hasattr(pbar, 'close'):
                    pbar.close()
                    
            except model_management.OOM_EXCEPTION as e:
                retry_count += 1
                current_tile_size = max(128, current_tile_size // 2)
                print(f"âš ï¸ å†…å­˜ä¸è¶³ï¼Œå‡å°ç“¦ç‰‡å¤§å°åˆ° {current_tile_size}x{current_tile_size} (é‡è¯• {retry_count}/{max_retries})")
                
                if current_tile_size < 128:
                    raise e
        
        if oom:
            raise model_management.OOM_EXCEPTION("æ— æ³•åœ¨å¯ç”¨å†…å­˜å†…å¤„ç†å›¾åƒ")
        
        # ä¼˜åŒ–ï¼šç”±äºtiled_scaleå·²ç›´æ¥è¾“å‡ºåˆ°GPUï¼Œç›´æ¥ä½¿ç”¨GPUåå¤„ç†
        print("ğŸ” æ£€æŸ¥è¾“å‡ºè®¾å¤‡çŠ¶æ€...")
        print(f"ğŸ“Š è¾“å‡ºå¼ é‡è®¾å¤‡: {s.device}, å½¢çŠ¶: {s.shape}")
        
        # ç¡®ä¿åœ¨GPUä¸Šè¿›è¡Œåå¤„ç†
        if s.device.type != 'cuda':
            print(f"ğŸ”„ å°†ç»“æœç§»åŠ¨åˆ°GPUè¿›è¡Œåå¤„ç† (å½“å‰è®¾å¤‡: {s.device})")
            s = s.to(device, non_blocking=True)
            print(f"âœ… ç»“æœå·²ç§»åŠ¨åˆ°GPU: {s.device}")
        
        # ä½¿ç”¨GPUåå¤„ç†
        s = self._gpu_post_process(s, device)
        
        return (s,)

    def _create_progress_bar(self, steps):
        """åˆ›å»ºè¿›åº¦æ¡"""
        if tqdm_available:
            return tqdm(total=steps, desc="å•GPUæ”¾å¤§å¤„ç†", unit="tile", leave=False)
        else:
            return comfy.utils.ProgressBar(steps)

    def _post_process_output(self, output_tensor):
        """ä¿®å¤ç¼–è¯‘æ¨¡å‹è¾“å‡ºå‘ç™½é—®é¢˜çš„åå¤„ç†"""
        print(f"ğŸ”§ å¼€å§‹å¢å¼ºåå¤„ç†ï¼Œè¾“å…¥è®¾å¤‡: {output_tensor.device}")
        print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - _post_process_output: è¾“å…¥è®¾å¤‡={output_tensor.device}")
        
        # è°ƒæ•´ç»´åº¦é¡ºåº
        s = output_tensor.movedim(-3, -1)
        print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - movedimå: è®¾å¤‡={s.device}")
        
        # å¤„ç†éæ•°å€¼
        s = torch.nan_to_num(s, nan=0.0, posinf=1.0, neginf=0.0)
        print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - nan_to_numå: è®¾å¤‡={s.device}")
        
        # è¯¦ç»†çš„æ•°å€¼ç»Ÿè®¡åˆ†æ
        s_min = torch.min(s)
        s_max = torch.max(s)
        s_mean = torch.mean(s)
        s_std = torch.std(s)
        
        print(f"ğŸ“Š åŸå§‹è¾“å‡ºç»Ÿè®¡ - æœ€å°å€¼: {s_min:.4f}, æœ€å¤§å€¼: {s_max:.4f}, å¹³å‡å€¼: {s_mean:.4f}, æ ‡å‡†å·®: {s_std:.4f}")
        
        # æ£€æµ‹ç¼–è¯‘æ¨¡å‹ç‰¹æœ‰çš„æ•°å€¼èŒƒå›´é—®é¢˜
        if s_max > 10.0 or s_min < -5.0:
            # ä¸¥é‡èŒƒå›´åç§» - ç¼–è¯‘æ¨¡å‹å¸¸è§é—®é¢˜
            print("âš ï¸ æ£€æµ‹åˆ°ä¸¥é‡æ•°å€¼èŒƒå›´åç§»ï¼Œè¿›è¡Œæ·±åº¦å½’ä¸€åŒ–")
            
            # æ–¹æ³•1: åŸºäºç»Ÿè®¡çš„å½’ä¸€åŒ–
            if s_std > 0.01:  # æœ‰åˆç†çš„åˆ†å¸ƒ
                # ä½¿ç”¨3-sigmaè§„åˆ™è£å‰ªå¼‚å¸¸å€¼
                lower_bound = s_mean - 3 * s_std
                upper_bound = s_mean + 3 * s_std
                s = torch.clamp(s, min=lower_bound, max=upper_bound)
                
                # é‡æ–°è®¡ç®—ç»Ÿè®¡é‡
                s_min = torch.min(s)
                s_max = torch.max(s)
            
            # æ–¹æ³•2: åˆ†ä½æ•°å½’ä¸€åŒ–ï¼ˆæ›´é²æ£’ï¼‰
            try:
                # ä½¿ç”¨åˆ†ä½æ•°é¿å…æç«¯å€¼å½±å“
                q_low = torch.quantile(s, 0.01)
                q_high = torch.quantile(s, 0.99)
                s = torch.clamp(s, min=q_low, max=q_high)
                
                # é‡æ–°è®¡ç®—ç»Ÿè®¡é‡
                s_min = torch.min(s)
                s_max = torch.max(s)
            except:
                pass  # åˆ†ä½æ•°è®¡ç®—å¤±è´¥æ—¶ä½¿ç”¨åŸæœ‰æ–¹æ³•
            
            # æœ€ç»ˆå½’ä¸€åŒ–åˆ°[0,1]
            if s_max - s_min > 1e-6:
                s = (s - s_min) / (s_max - s_min)
            else:
                s = torch.zeros_like(s)  # å…¨é›¶æƒ…å†µ
        
        elif s_max > 1.0 or s_min < 0.0:
            # è½»å¾®èŒƒå›´åç§»
            print("âš ï¸ æ£€æµ‹åˆ°è½»å¾®æ•°å€¼åç§»ï¼Œè¿›è¡Œè£å‰ªå½’ä¸€åŒ–")
            
            # é™åˆ¶åˆ°åˆç†èŒƒå›´
            s = torch.clamp(s, min=0.0, max=s_max)
            
            # å¦‚æœæœ€å¤§å€¼ä»ç„¶å¤§äº1ï¼Œè¿›è¡Œç¼©æ”¾
            if s_max > 1.0:
                s = s / s_max
        
        else:
            # æ­£å¸¸èŒƒå›´ï¼Œç›´æ¥é™åˆ¶
            s = torch.clamp(s, min=0.0, max=1.0)
        
        # æœ€ç»ˆç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
        s = torch.clamp(s, min=0.0, max=1.0)
        
        # æœ€ç»ˆç»Ÿè®¡éªŒè¯
        final_min = torch.min(s)
        final_max = torch.max(s)
        final_mean = torch.mean(s)
        
        print(f"âœ… å¤„ç†åç»Ÿè®¡ - æœ€å°å€¼: {final_min:.4f}, æœ€å¤§å€¼: {final_max:.4f}, å¹³å‡å€¼: {final_mean:.4f}")
        print(f"ğŸ”§ å¢å¼ºåå¤„ç†å®Œæˆï¼Œè¾“å‡ºè®¾å¤‡: {s.device}")
        
        return s

    def _accurate_memory_assessment(self, output_tensor, device):
        """ä¼˜åŒ–çš„æ˜¾å­˜è¯„ä¼° - åŸºäºå®é™…å¼ é‡ï¼Œä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼"""
        # ä½¿ç”¨å®é™…å¼ é‡è®¡ç®—æ˜¾å­˜éœ€æ±‚
        output_memory = output_tensor.nelement() * output_tensor.element_size()
        
        # è·å–å½“å‰æ˜¾å­˜çŠ¶æ€
        if hasattr(torch.cuda, 'get_device_properties'):
            total_memory = torch.cuda.get_device_properties(device).total_memory
            allocated = torch.cuda.memory_allocated(device)
            
            # è®¡ç®—çœŸæ­£çš„å¯ç”¨æ˜¾å­˜ï¼šæ€»æ˜¾å­˜ - å·²åˆ†é…æ˜¾å­˜
            actual_available_memory = total_memory - allocated
            
            # ä¼˜åŒ–ï¼šæ ¹æ®æ€»æ˜¾å­˜å¤§å°åŠ¨æ€è°ƒæ•´å®‰å…¨ä½™é‡
            if total_memory >= 20 * 1024**3:  # 20GBä»¥ä¸Šå¤§æ˜¾å­˜æ˜¾å¡
                safety_margin = 2 * 1024**3  # 2GB
            else:
                safety_margin = 4 * 1024**3  # 4GB
                
            available_memory = actual_available_memory - safety_margin
            
            print(f"ğŸ’¾ ä¼˜åŒ–æ˜¾å­˜è¯„ä¼° - è¾“å‡ºå¼ é‡å½¢çŠ¶: {output_tensor.shape}")
            print(f"ğŸ’¾ ä¼˜åŒ–æ˜¾å­˜è¯„ä¼° - å…ƒç´ æ•°é‡: {output_tensor.nelement()}")
            print(f"ğŸ’¾ ä¼˜åŒ–æ˜¾å­˜è¯„ä¼° - å…ƒç´ å¤§å°: {output_tensor.element_size()} å­—èŠ‚")
            print(f"ğŸ’¾ ä¼˜åŒ–æ˜¾å­˜è¯„ä¼° - æ€»æ˜¾å­˜: {total_memory/1024**3:.2f}GB")
            print(f"ğŸ’¾ ä¼˜åŒ–æ˜¾å­˜è¯„ä¼° - å·²åˆ†é…: {allocated/1024**3:.2f}GB")
            print(f"ğŸ’¾ ä¼˜åŒ–æ˜¾å­˜è¯„ä¼° - å®é™…å¯ç”¨: {actual_available_memory/1024**3:.2f}GB")
            print(f"ğŸ’¾ ä¼˜åŒ–æ˜¾å­˜è¯„ä¼° - å®‰å…¨ä½™é‡åå¯ç”¨: {available_memory/1024**3:.2f}GB")
            print(f"ğŸ’¾ ä¼˜åŒ–æ˜¾å­˜è¯„ä¼° - è¾“å‡ºéœ€æ±‚: {output_memory/1024**3:.2f}GB")
            
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´å®½æ¾çš„æ£€æŸ¥æ¡ä»¶
            # æ¡ä»¶1ï¼šå¯ç”¨æ˜¾å­˜è¶³å¤Ÿå®¹çº³è¾“å‡ºå¼ é‡
            # æ¡ä»¶2ï¼šè¾“å‡ºå¼ é‡ä¸è¶…è¿‡æ€»æ˜¾å­˜çš„60%
            memory_condition = available_memory >= output_memory
            threshold_condition = output_memory <= total_memory * 0.6
            
            result = memory_condition and threshold_condition
            
            if result:
                print("âœ… æ˜¾å­˜è¯„ä¼°é€šè¿‡ï¼Œå¯ä»¥ä½¿ç”¨GPUå¤„ç†")
            else:
                print("âŒ æ˜¾å­˜è¯„ä¼°æœªé€šè¿‡ï¼Œä½¿ç”¨CPUå¤„ç†")
                
            return result
            
        return False

    def _ensure_gpu_processing(self, tensor, device):
        """ç¡®ä¿å¼ é‡åœ¨GPUä¸Šå¤„ç†"""
        if tensor.device.type != 'cuda':
            print(f"ğŸ”„ å°†å¼ é‡ä» {tensor.device} ç§»åŠ¨åˆ° GPU")
            return tensor.to(device, non_blocking=True)
        return tensor

    def _gpu_post_process(self, output_tensor, device):
        """GPUä¸Šçš„åå¤„ç†"""
        print(f"ğŸ”§ å¼€å§‹GPUå¢å¼ºåå¤„ç†ï¼Œè¾“å…¥è®¾å¤‡: {output_tensor.device}")
        
        # ç¡®ä¿è¾“å…¥åœ¨GPUä¸Š
        output_tensor = self._ensure_gpu_processing(output_tensor, device)
        
        # è°ƒæ•´ç»´åº¦é¡ºåº
        s = output_tensor.movedim(-3, -1)
        print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - GPU movedimå: è®¾å¤‡={s.device}")
        
        # å¤„ç†éæ•°å€¼
        s = torch.nan_to_num(s, nan=0.0, posinf=1.0, neginf=0.0)
        print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - GPU nan_to_numå: è®¾å¤‡={s.device}")
        
        # è¯¦ç»†çš„æ•°å€¼ç»Ÿè®¡åˆ†æ
        s_min = torch.min(s)
        s_max = torch.max(s)
        s_mean = torch.mean(s)
        s_std = torch.std(s)
        
        print(f"ğŸ“Š GPUåŸå§‹è¾“å‡ºç»Ÿè®¡ - æœ€å°å€¼: {s_min:.4f}, æœ€å¤§å€¼: {s_max:.4f}, å¹³å‡å€¼: {s_mean:.4f}, æ ‡å‡†å·®: {s_std:.4f}")
        
        # æ£€æµ‹ç¼–è¯‘æ¨¡å‹ç‰¹æœ‰çš„æ•°å€¼èŒƒå›´é—®é¢˜
        if s_max > 10.0 or s_min < -5.0:
            # ä¸¥é‡èŒƒå›´åç§» - ç¼–è¯‘æ¨¡å‹å¸¸è§é—®é¢˜
            print("âš ï¸ GPUæ£€æµ‹åˆ°ä¸¥é‡æ•°å€¼èŒƒå›´åç§»ï¼Œè¿›è¡Œæ·±åº¦å½’ä¸€åŒ–")
            
            # æ–¹æ³•1: åŸºäºç»Ÿè®¡çš„å½’ä¸€åŒ–
            if s_std > 0.01:  # æœ‰åˆç†çš„åˆ†å¸ƒ
                # ä½¿ç”¨3-sigmaè§„åˆ™è£å‰ªå¼‚å¸¸å€¼
                lower_bound = s_mean - 3 * s_std
                upper_bound = s_mean + 3 * s_std
                s = torch.clamp(s, min=lower_bound, max=upper_bound)
                
                # é‡æ–°è®¡ç®—ç»Ÿè®¡é‡
                s_min = torch.min(s)
                s_max = torch.max(s)
            
            # æ–¹æ³•2: åˆ†ä½æ•°å½’ä¸€åŒ–ï¼ˆæ›´é²æ£’ï¼‰
            try:
                # ä½¿ç”¨åˆ†ä½æ•°é¿å…æç«¯å€¼å½±å“
                q_low = torch.quantile(s, 0.01)
                q_high = torch.quantile(s, 0.99)
                s = torch.clamp(s, min=q_low, max=q_high)
                
                # é‡æ–°è®¡ç®—ç»Ÿè®¡é‡
                s_min = torch.min(s)
                s_max = torch.max(s)
            except:
                pass  # åˆ†ä½æ•°è®¡ç®—å¤±è´¥æ—¶ä½¿ç”¨åŸæœ‰æ–¹æ³•
            
            # æœ€ç»ˆå½’ä¸€åŒ–åˆ°[0,1]
            if s_max - s_min > 1e-6:
                s = (s - s_min) / (s_max - s_min)
            else:
                s = torch.zeros_like(s)  # å…¨é›¶æƒ…å†µ
        
        elif s_max > 1.0 or s_min < 0.0:
            # è½»å¾®èŒƒå›´åç§»
            print("âš ï¸ GPUæ£€æµ‹åˆ°è½»å¾®æ•°å€¼åç§»ï¼Œè¿›è¡Œè£å‰ªå½’ä¸€åŒ–")
            
            # é™åˆ¶åˆ°åˆç†èŒƒå›´
            s = torch.clamp(s, min=0.0, max=s_max)
            
            # å¦‚æœæœ€å¤§å€¼ä»ç„¶å¤§äº1ï¼Œè¿›è¡Œç¼©æ”¾
            if s_max > 1.0:
                s = s / s_max
        
        else:
            # æ­£å¸¸èŒƒå›´ï¼Œç›´æ¥é™åˆ¶
            s = torch.clamp(s, min=0.0, max=1.0)
        
        # æœ€ç»ˆç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
        s = torch.clamp(s, min=0.0, max=1.0)
        
        # æœ€ç»ˆç»Ÿè®¡éªŒè¯
        final_min = torch.min(s)
        final_max = torch.max(s)
        final_mean = torch.mean(s)
        
        print(f"âœ… GPUå¤„ç†åç»Ÿè®¡ - æœ€å°å€¼: {final_min:.4f}, æœ€å¤§å€¼: {final_max:.4f}, å¹³å‡å€¼: {final_mean:.4f}")
        print(f"ğŸ”§ GPUå¢å¼ºåå¤„ç†å®Œæˆï¼Œè¾“å‡ºè®¾å¤‡: {s.device}")
        
        return s

    def _smart_memory_management(self, result, upscale_model, device):
        """æ™ºèƒ½æ˜¾å­˜ç®¡ç†ï¼šæ ¹æ®æ˜¾å­˜æƒ…å†µå†³å®šè¾“å‡ºè®¾å¤‡"""
        print("ğŸ” å¼€å§‹æ™ºèƒ½æ˜¾å­˜ç®¡ç†æ£€æŸ¥...")
        print(f"ğŸ” è®¾å¤‡è·Ÿè¸ª - _smart_memory_managementå…¥å£: è¾“å…¥è®¾å¤‡={result[0].device if result else 'None'}")
        
        if result is None or len(result) == 0:
            print("â“ ç»“æœä¸ºç©ºï¼Œè·³è¿‡æ˜¾å­˜ç®¡ç†")
            return result
            
        output_tensor = result[0]
        print(f"ğŸ“Š è¾“å‡ºå¼ é‡è®¾å¤‡: {output_tensor.device}, å½¢çŠ¶: {output_tensor.shape}")
        
        if output_tensor.device.type != 'cuda':
            print(f"ğŸ“‹ è¾“å‡ºå¼ é‡å·²åœ¨ {output_tensor.device}ï¼Œè·³è¿‡æ˜¾å­˜ç®¡ç†")
            return result
        
        try:
            # è®¡ç®—è¾“å‡ºå¼ é‡çš„æ˜¾å­˜éœ€æ±‚
            output_memory = output_tensor.nelement() * output_tensor.element_size()
            print(f"ğŸ“Š è¾“å‡ºå¼ é‡æ˜¾å­˜éœ€æ±‚: {output_memory/1024**3:.2f}GB")
            
            # è·å–å½“å‰GPUæ˜¾å­˜çŠ¶æ€
            if hasattr(torch.cuda, 'memory_reserved'):
                reserved = torch.cuda.memory_reserved(device)
                allocated = torch.cuda.memory_allocated(device)
                
                # è·å–æ€»æ˜¾å­˜å’Œå¯ç”¨æ˜¾å­˜
                if hasattr(torch.cuda, 'get_device_properties'):
                    total_memory = torch.cuda.get_device_properties(device).total_memory
                    # è®¡ç®—çœŸæ­£çš„å¯ç”¨æ˜¾å­˜ï¼šæ€»æ˜¾å­˜ - å·²åˆ†é…æ˜¾å­˜
                    actual_available_memory = total_memory - allocated
                    
                    # å®‰å…¨ä½™é‡ï¼šä¿ç•™2GBçš„æ˜¾å­˜ç”¨äºåç»­æ“ä½œ
                    safety_margin = 2 * 1024**3  # 2GB
                    available_memory = actual_available_memory - safety_margin
                    
                    print(f"ğŸ’¾ æ˜¾å­˜çŠ¶æ€ - æ€»æ˜¾å­˜: {total_memory/1024**3:.2f}GB, å·²åˆ†é…: {allocated/1024**3:.2f}GB")
                    print(f"ğŸ’¾ å¯ç”¨æ˜¾å­˜è®¡ç®— - å®é™…å¯ç”¨: {actual_available_memory/1024**3:.2f}GB, å®‰å…¨ä½™é‡å: {available_memory/1024**3:.2f}GB")
                    print(f"ğŸ“Š è¾“å‡ºå¼ é‡éœ€æ±‚: {output_memory/1024**3:.2f}GB")
                    
                    # å¦‚æœå¯ç”¨æ˜¾å­˜è¶³å¤Ÿï¼Œç›´æ¥ä¿ç•™åœ¨GPUä¸Š
                    if available_memory >= output_memory:
                        print("ğŸš€ æ˜¾å­˜å……è¶³ï¼Œç»“æœä¿ç•™åœ¨GPUç›´æ¥å¯¼å‡º")
                        return result
                    else:
                        print("ğŸ’¾ æ˜¾å­˜ä¸è¶³ï¼Œç»“æœç§»åŠ¨åˆ°CPUå¯¼å‡º")
                        # å¼‚æ­¥ç§»åŠ¨åˆ°CPUï¼Œå‡å°‘é˜»å¡æ—¶é—´
                        with torch.cuda.stream(torch.cuda.Stream(device)):
                            cpu_tensor = output_tensor.cpu()
                        print("âœ… ç»“æœå·²ç§»åŠ¨åˆ°CPU")
                        return (cpu_tensor,)
                else:
                    # å¦‚æœæ²¡æœ‰è·å–æ€»æ˜¾å­˜åŠŸèƒ½ï¼Œä½¿ç”¨æ—§çš„é€»è¾‘
                    free_memory = reserved - allocated
                    safety_margin = reserved * 0.2
                    available_memory = free_memory - safety_margin
                    
                    print(f"ğŸ’¾ æ˜¾å­˜çŠ¶æ€ (æ—§æ–¹æ³•) - å·²åˆ†é…: {allocated/1024**3:.2f}GB, ä¿ç•™: {reserved/1024**3:.2f}GB, å¯ç”¨: {available_memory/1024**3:.2f}GB")
                    
                    if available_memory >= output_memory:
                        print("ğŸš€ æ˜¾å­˜å……è¶³ï¼Œç»“æœä¿ç•™åœ¨GPUç›´æ¥å¯¼å‡º")
                        return result
                    else:
                        print("ğŸ’¾ æ˜¾å­˜ä¸è¶³ï¼Œç»“æœç§»åŠ¨åˆ°CPUå¯¼å‡º")
                        with torch.cuda.stream(torch.cuda.Stream(device)):
                            cpu_tensor = output_tensor.cpu()
                        print("âœ… ç»“æœå·²ç§»åŠ¨åˆ°CPU")
                        return (cpu_tensor,)
            else:
                # å¦‚æœæ²¡æœ‰æ˜¾å­˜æŸ¥è¯¢åŠŸèƒ½ï¼Œä¿å®ˆç­–ç•¥ï¼šç§»åŠ¨åˆ°CPU
                print("ğŸ’¾ æ— æ³•è·å–æ˜¾å­˜ä¿¡æ¯ï¼Œç»“æœç§»åŠ¨åˆ°CPUå¯¼å‡º")
                with torch.cuda.stream(torch.cuda.Stream(device)):
                    cpu_tensor = output_tensor.cpu()
                return (cpu_tensor,)
                
        except Exception as e:
            print(f"âš ï¸ æ˜¾å­˜ç®¡ç†å¼‚å¸¸ï¼Œä½¿ç”¨ä¿å®ˆç­–ç•¥: {e}")
            # å¼‚å¸¸æƒ…å†µä¸‹ä½¿ç”¨ä¿å®ˆç­–ç•¥
            with torch.cuda.stream(torch.cuda.Stream(device)):
                cpu_tensor = output_tensor.cpu()
            return (cpu_tensor,)

    @classmethod
    def IS_CHANGED(s, **kwargs):
        return float("NaN")


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "UpscaleModelLoader": UpscaleModelLoader,
    "ImageUpscaleWithModelCUDAspeedFixed": ImageUpscaleWithModelCUDAspeedFixed
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ImageUpscaleWithModelCUDAspeedFixed": "ğŸš€ Upscale Image CUDAspeed",
}